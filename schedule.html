<!DOCTYPE html>
<html lang="en">
<head>
	<meta charset="utf-8">
  <link rel="shortcut icon" href="favicon.ico" type="image/x-icon">
	<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no, content=no-cache">

	<!-- Bootstrap CSS -->
	<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta.3/css/bootstrap.min.css" integrity="sha384-Zug+QiDoJOrZ5t4lssLdxGhVrurbmBWopoEl+M6BdEfwnCJZtKxi1KgxUyJq13dy" crossorigin="anonymous">

  <title>Big Data and Machine Learning Systems</title>
</head>

<body>
<br>
    <div class="container">
      <header class="header clearfix">
        <nav>
          <ul class="nav nav-pills float-left">
            <li class="nav-item">
              <a class="nav-link " href=".">Home<span class="sr-only">(current)</span></a>
            </li>
            <li class="nav-item">
              <a class="nav-link active" href="schedule.html">Schedule</a>
            </li>
            <li class="nav-item">
		    <a class="nav-link" href="labs">Lab</a>
            </li>
          </ul>
        </nav>
	<div class="row">
		<div class="col-sm-4"><a href="https://xkcd.com/1838/"><img src="mlsys-logo.jpg" width=180></a></div>
		<div class="col-sm-8">
		<div class="row">
      <div style="color:#CD5C5C;">
        <h3 >Big Data and Machine Learning Systems</h3>
      </div>
		</div>
		<div class="row">
      CSCI-GA.3033(077), Fall 2025
		</div>
		</div>
	</div>
      </header>

<style>
table tbody tr:hover {
    background-color: #d6eaf8; /* Light blue on hover */
}
table thead tr {
}
table {
  font-size:16px;
}
</style>


          <div class="container" style="margin-top:5px">
        <div class="row">
            <div class="col-md-12">
                <table class="table">
                    <thead>
                        <tr>
                            <th style="width:7em">Date</th>
                            <th>Topic</th>
                        </tr>
                    </thead>
                    <tbody >
                        <tr>
                            <td>Week 1 (9/3)</td>
                            <td><b><p>Introduction, the Model</p></b>
                                  Optional Readings:
                                  <ul>
                                    <li><a href="https://www.deeplearningbook.org/contents/mlp.html">Deep Learning Book Chap 6.1-6.4</li>
                                    <li><a href="http://ccr.sigcomm.org/online/files/p83-keshavA.pdf">How to read a paper</a></li>
                                  </ul>
                            </td>
                        </tr>
                        <tr>
                            <td>Week 2 (9/10)</td>
                            <td><b><p>The Frameworks</p></b>
                                Required Readings:
                                <ul>
                                  <li><a href="https://www.usenix.org/system/files/conference/osdi16/osdi16-abadi.pdf">TensorFlow: A System for Large-Scale Machine Learning</a></li>
                                  <li><a href="https://arxiv.org/abs/1912.01703">PyTorch: An Imperative Style, High-Performance Deep Learning Library</a></li>
                                </ul>
                            </td>
                        </tr>


                        <tr>
                            <td>Week 3 (9/17)</td>
                            <td><b><p>The Hardware</p></b>
                                Required Readings:
                                <ul>
                                  <li><a href="https://developer.nvidia.com/blog/even-easier-introduction-cuda">An even easier introduction to CUDA</a></li>
                                </ul>
                                Optional:
                                <ul>
                                  <li><a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide">Cuda C++ Programming Guide</a>(Sec 2 and 3)</li>
                                </ul>
                            </ul>
                            </td>
                        </tr>


                        <tr>
                            <td>Week 4 (9/24)</td>
                            <td><b><p>Compilation and Optimization I</p></b>
                                Required Readings:
                                <ul>
                                  <li><a href="https://pytorch.org/assets/pytorch2-2.pdf">PyTorch 2: Faster Machine Learning Through Dynamic Python Bytecode Transformation and Graph Compilation</a></li>
                                  <li><a href="https://www.eecs.harvard.edu/~htk/publication/2019-mapl-tillet-kung-cox.pdf">Triton: An Intermediate Language and Compiler for Tiled Neural Network Computations</a></li>

                                </ul>
                                Optional:
                                <ul>
                                  <li><a href="https://dl.acm.org/doi/pdf/10.1145/3575693.3576933">TensorIR: An Abstraction for Automatic Tensorized Program Optimization</a></li>
                                  <li><a href="https://arxiv.org/abs/2210.09603">Hidet: Task-Mapping Programming Paradigm for Deep Learning Tensor Programs</a></li>
                                  <!---<li><a href="https://arxiv.org/pdf/2202.03293.pdf">Composable and Modular Code Generation in MLIR</a></li>-->
                                </ul>
                            </ul>
                            </td>
                        </tr>

                        <tr>
                            <td>Week 5 (10/1)</td>
                            <td><b><p>Compilation and Optimization II</p></b>
                                Required Readings:
                                <ul>
                                  <li><a href="https://www.usenix.org/system/files/osdi18-chen.pdf">TVM: An Automated End-to-End Optimizing Compiler for Deep Learning</a></li>
                                  <li><a href="https://www.usenix.org/system/files/osdi20-zheng.pdf">Ansor: Generating High-Performance Tensor Programs for Deep Learning</a></li>
                                  <li><a href="https://cs.stanford.edu/~padon/taso-sosp19.pdf">TASO: Optimizing Deep Learning Computation with Automatic Generation of Graph Substitutions</a></li>
                                </ul>
                                Optional:
                                <ul>
				  <li><a href="https://arxiv.org/abs/1805.08166">Learning to Optimize Tensor Programs</a></li>
                                  <li><a href="https://dl.acm.org/doi/10.1145/3453483.3454083">DNNFusion: accelerating deep neural networks execution with advanced operator fusion</a></li>
                                  <li><a href="https://www.usenix.org/conference/osdi21/presentation/wang">Pet: Optimizing Tensor Programs with Partially Equivalent Transformations and Automated Corrections</a></li>
                                </ul>
                            </ul>
                            </td>
                        </tr>

                        <tr>
                            <td>Week 6 (10/8)</td>
                            <td><b><p>Training I: data vs. model parallelism</p></b>
                                Required Readings:
                                <ul>
                                  <li><a href="https://arxiv.org/pdf/2006.15704.pdf">PyTorch Distributed: Experiences on Accelerating Data Parallel Training</a></li>
                                  <li><a href="https://news.cs.nyu.edu/~jinyang/pub/tofu-eurosys19.pdf">Supporting Very Large Models using Automatic Dataflow Graph Partitioning</a></li>
				  <li><a href="https://arxiv.org/pdf/1910.02054">ZeRO: Memory Optimizations Toward Training Trillion Parameter Models</a></li>
				</ul>
                                Optional:
                                <ul>
                                 <!-- <li><a href="https://arxiv.org/pdf/1404.5997.pdf">One weird trick for parallelizing convolutional neural networks</a></li>-->
                              	  <li><a href="https://arxiv.org/pdf/2304.11277.pdf">PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel</a></li>
                                </ul>
                            </ul>
                            </td>
                        </tr>

                        
                        <tr>
                            <td>Week 7 (10/15)</td>
                            <td><b><p>Training II: pipeline parallelism</p></b>
                                Required Readings:
                                <ul>
                                  <li><a href="https://arxiv.org/abs/1811.06965">GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism</a></li>
                                  <li><a href="https://deepakn94.github.io/assets/papers/pipedream-sosp19.pdf">PipeDream: Fast and Efficient Pipeline Parallel DNN Training</a></li>
                                </ul>
                                Optional:
                                <ul>
                                  <li><a href="https://proceedings.mlr.press/v139/narayanan21a/narayanan21a.pdf">Memory-Efficient Pipeline-Parallel DNN Training</a></li>
                                </ul>
                            </ul>
                            </td>
                        </tr>
                      

                        <tr>
                            <td>Week 8 (10/22)</td>
                            <td><b><p>Training III: automatic parallelization and optimization</p></b>
                                Required Readings:
                                <ul>
                                  <li><a href="https://www.usenix.org/conference/osdi22/presentation/zheng-lianmin">Alpa: Automating Inter- and Intra-Operator Parallelism for Distributed Deep Learning</a></li>
                              <li><a href="https://dl.acm.org/doi/pdf/10.1145/3567955.3567959">Overlap Communication with Dependent Computation via Decomposition in Large Deep Learning Models</a></li>
                                </ul>
                                Optional:
                                <ul>
                                  <li><a href="https://arxiv.org/abs/2105.04663">GSPMD: General and Scalable Parallelization for ML Computation Graphs</a></li>
                                  <li><a href="https://www.usenix.org/system/files/osdi22-unger.pdf">Unity: Accelerating DNN Training Through Joint Optimization of Algebraic Transformations and Parallelization</a></li>
                                </ul>
                            </ul>
                            </td>
                        </tr>


                    <tr>
                     <td>Week 9 (10/29)</td>
                            <td><b><p>LLM Background</p></b>
                            Required Readings:
                            <ul>
                              <li><a href="https://arxiv.org/pdf/1706.03762.pdf">Attention Is All You Need</a></li>
                              <li><a href="https://arxiv.org/abs/2307.08691">FlashAttention-2:Faster Attention with Better Parallelism and Work Partitioning</a></li>
                              <li><a href="https://arxiv.org/pdf/1909.08053.pdf">Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism</a></li>
                            </ul>
                            Optional:
                            <ul>
                              <li> <a href="https://arxiv.org/pdf/2304.00612.pdf">Eight Things to Know about Large Language Models</a></li>
                            </ul>
                            </td>
                    </tr>
                    <tr>
<tr>
                     <td>Week 10 (11/5)</td>
                            <td><b><p>Training LLMs I</p></b>
                            Required Readings:
                            <ul>
                              <li><a href="https://arxiv.org/pdf/1909.08053.pdf">Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism</a></li>
			      <li><a href="https://arxiv.org/abs/2509.16293">Robust LLM Training Infrastructure at ByteDance</a></li>
                            </ul>
                            Optional:
                            <ul>
                              <li><a href="https://arxiv.org/abs/2402.15627">MegaScale: Scaling Large Language Model Training to More Than 10,000 GPUs</a></li>
                              <li><a href="https://arxiv.org/pdf/2104.04473.pdf">Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM</a></li>
                              <li><a href="https://arxiv.org/pdf/2205.05198.pdf">Reducing Activation Recomputation in Large Transformer Models</a></li>
                            </ul>
                            </td>
                    </tr>

                    <td>Week 11 (11/12)</td>
                           <td><b><p>Inference:</p></b>
                             Required readings:
                             <ul>
                               <li><a href="https://www.usenix.org/system/files/osdi22-yu.pdf">Orca: A Distributed Serving System for Transformer-Based Generative Models</a></li>
                               <li><a href="https://arxiv.org/pdf/2309.06180.pdf">Efficient Memory Management for Large Language Model Serving with PagedAttention</a></li>
                             </ul>
                             Optional:
                             <ul>
                               <li><a href="https://arxiv.org/abs/2211.17192">Fast Inference from Transformers via Speculative Decoding</a></li>
                             <li><a href="https://arxiv.org/abs/2302.01318">Accelerating large language model decoding with speculative sampling</a></li>
                             </ul>
                           </td>
                    </tr>
                    
                    <tr>
                           <td>Week 12 (11/19)</td>
                           <td><b><p>Quantization</p></b>
                            Required Readings:
                            <ul>
                              <li><a href="https://arxiv.org/pdf/2208.07339.pdf">LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale</a></li>
                              <li><a href="https://arxiv.org/pdf/2211.10438.pdf">SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models</a></li>
                              <li><a href="https://arxiv.org/abs/2305.14314">QLoRA: Efficient Finetuning of Quantized LLMs</a></li>
                            </ul>
                            Optional:
                            <ul>
                              <li><a href="https://arxiv.org/abs/1712.05877">Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference</a></li>
                            <li><a href="https://arxiv.org/pdf/2210.17323.pdf">GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers</a></li>
                            <li><a href="https://arxiv.org/pdf/2310.19102.pdf">Atom: Low-bit Quantization for Efficient and Accurate LLM Serving</a></li>
                            </ul>
                           </td>
                    </tr>
                    <tr>
                           <td>Week 13 (11/26)</td>
                           <td><b><p>No class (Legislative Friday)</p></b>
		</tr>

                    <tr>
                           <td>Week 14 (12/3)</td>
                           <td><b><p>Sparsification</p></b>
                             Required Readings:
                            <ul>
                              <li><a href="https://arxiv.org/abs/1510.00149">Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding</a></li>
                              <li><a href="https://arxiv.org/pdf/2301.00774.pdf">SparseGPT: Massive Language Models Can be Accurately Pruned in One-Shot</a></li> 
                            </ul>
                           Optional:
                           <ul>
                             <li><a href="https://proceedings.neurips.cc/paper_files/paper/1989/file/6c9882bbac1c7093bd25041881277658-Paper.pdf">Optimal Brain Damage</a></li>
                             <li><a href="https://arxiv.org/abs/1803.03635">The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks</a></li>
                    </tr>
                           </td>

                    <tr>
                           <td>Week 15 (12/10)</td>
                           <td><b><p>Project Presentation</p></b>
                           </td>
                    </tr>


                        <!-- Add more rows as needed -->
                    </tbody>
                </table>
            </div>
        </div>
    </div>

    <!-- Add Bootstrap JS and jQuery scripts -->
    <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.9.1/dist/umd/popper.min.js"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js"></script>
</body>
</html>
